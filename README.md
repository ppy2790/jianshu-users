> 这里没有用到rule进行抓取

文章见 http://www.jianshu.com/p/bb8413085c77

实现一个爬虫的关键，我理解下来有两点：

一是url分析，就是从哪里进入，经过哪些路径（列表页，分页），新增url在哪里添加，这些关系到一个数据完整的链路。

二是页面源代码分析，解析出需要的数据（包括一个完整的数据在哪几个页面上获取）。主要弄清文档结构，数据的选取点放在哪里。

搞清楚这两个要点后，实现一个爬虫的功能不难，无论是采用Scrapy框架，还是用Python+正则，或BeautifulSoup的方式。

爬虫的入口选择的是，简书签约作者的主页，采取抓取简书签约作者的粉丝，来最大量获取简书上的用户及用户信息。

简书签约作者主页是前一次用一个爬虫来抓取的，文章：http://www.jianshu.com/p/1d1a2b91c2b8
